{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple ode solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EulerSolve(t0, t1, x0, f, dt = 0.01, reverse=False):\n",
    "    #f(z, t)\n",
    "    \n",
    "    ts = np.arange(t0, t1, dt)\n",
    "    if reverse:\n",
    "        ts = np.arange(t1, t0, -dt)\n",
    "    \n",
    "    z = x0.copy()\n",
    "    \n",
    "    for t in ts:\n",
    "        \n",
    "        if not reverse:\n",
    "            z+= dt*f(z, t)\n",
    "        else:\n",
    "            z-= dt*f(z, t)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EulerODEPath(t0, t1, x0, f, dt = 0.01, reverse=False):\n",
    "    #f(z, t)\n",
    "    #gives the xs, ys for the path\n",
    "    \n",
    "    points = []\n",
    "    \n",
    "    ts = np.arange(t0, t1, dt)\n",
    "    if reverse:\n",
    "        ts = np.arange(t1, t0, -dt)\n",
    "    \n",
    "    z = x0.copy()\n",
    "    \n",
    "    for t in ts:\n",
    "        \n",
    "        if not reverse:\n",
    "            z+= dt*f(z, t)\n",
    "        else:\n",
    "            z-= dt*f(z, t)\n",
    "        \n",
    "        points.append(z.copy())\n",
    "    \n",
    "    xs, ys = zip(*points)\n",
    "    \n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first define a class to hold the neural network component of the dynamical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #lets try a simple model\n",
    "        self.f = nn.Linear(2, 2, bias=False)\n",
    "        self.p_shapes = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "    def get_jacobian(self, x, out_type='numpy'):\n",
    "        \"\"\"\n",
    "        This is a sort of a misapplication of the \n",
    "        use of batches. Here we make a batch of \n",
    "        n copies of the data, one for each \n",
    "        of the function outputs. We take the gradient\n",
    "        of each of the outputs, and recombine to get\n",
    "        the jacobian\n",
    "        \"\"\"\n",
    "        #x should be a torch tensor of shape\n",
    "        #(len x + len a + len a_p)\n",
    "        \n",
    "        noutputs = x.shape[-1]\n",
    "        \n",
    "        x = x.repeat(noutputs, 1)\n",
    "        \n",
    "        x.requires_grad_(True)\n",
    "        #y = self.f(x)\n",
    "        y = self.forward(x)\n",
    "        \n",
    "        y.backward(torch.eye(noutputs))\n",
    "        \n",
    "        if out_type == 'numpy':\n",
    "            return x.grad.data.detach().numpy()\n",
    "        else:\n",
    "            return x.grad.data\n",
    "    \n",
    "    def get_parameter_jacobian(self, x, out_type='numpy'):\n",
    "        \"\"\"\n",
    "        I dont know a way to do this rapidly\n",
    "        lets do a repetition for each output\n",
    "        \"\"\"\n",
    "        \n",
    "        noutputs = x.shape[-1]\n",
    "        param_grads = []\n",
    "        \n",
    "        for out_index in range(noutputs):\n",
    "            \n",
    "            y=self.forward(x)\n",
    "            \n",
    "            #set grads to zero so they dont accumulate\n",
    "            for p in self.parameters():\n",
    "                if type(p.grad) != type(None):\n",
    "                    p.grad.zero_()\n",
    "            \n",
    "            #backprop\n",
    "            y[out_index].backward()\n",
    "            \n",
    "            #now get grads and flatten them \n",
    "            param_grads.append(self.flatten_param_grads())\n",
    "        \n",
    "        if out_type == 'numpy':\n",
    "            return torch.stack(param_grads).detach().numpy()\n",
    "        else:\n",
    "            return torch.stack(param_grads)\n",
    "    \n",
    "    def flatten_param_grads(self):\n",
    "        \"\"\"\n",
    "        return the grads of the parameters\n",
    "        which have been systematically and \n",
    "        replicably flattened\n",
    "        \"\"\"\n",
    "        self.p_shapes = []\n",
    "        self.flat_params = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters():\n",
    "                self.p_shapes.append(p.size())\n",
    "                self.flat_params.append(p.grad.flatten())\n",
    "        \n",
    "        return torch.cat(self.flat_params)\n",
    "    \n",
    "    def unflatten_param_grads(self, grads):\n",
    "        \"\"\"\n",
    "        restore the param grads to their original shapes\n",
    "        \n",
    "        set param grads equal to grads\n",
    "        \n",
    "        grads can be either array or tensor\n",
    "        this converts them to tensors before working with them\n",
    "        \"\"\"\n",
    "        assert(len(grads.shape)==1)\n",
    "        \n",
    "        #ensure we have a tensor\n",
    "        grads = torch.tensor(grads, dtype=torch.float32)\n",
    "        \n",
    "        count=0\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.parameters()):\n",
    "\n",
    "                shape = self.p_shapes[i]\n",
    "                size = reduce(operator.mul, shape, 1)\n",
    "                \n",
    "                #to be safe, if this has a gradient already, \n",
    "                #remove it\n",
    "                if type(p.grad)!= type(None):\n",
    "                    p.grad.zero_()\n",
    "                \n",
    "                #now assign it the appropriate gradient\n",
    "                p.grad = grads[count:count+size].view(shape)\n",
    "                \n",
    "                count += size\n",
    "        assert(count == len(grads))\n",
    "        \n",
    "    def descend(self, lr):\n",
    "        \"\"\"\n",
    "        descend weights by grads multiplied by dx\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                param -= param.grad*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ControlDynamicNN(DynamicNN, cost_function, x_size):\n",
    "    \"\"\"\n",
    "    given a DynamicNN, this function extends the\n",
    "    dynamic function, DynamicNN.f, by one term which \n",
    "    gives the dynamics of the cost function\n",
    "    \n",
    "    eg:\n",
    "    dz/dt = f(z, t, param)\n",
    "    L = integrate[torch.sqrt(torch.dot(f(x(t)), f(x(t))), {t, t0, t1}]\n",
    "    \n",
    "    therefore given DynamicNN whose dynamic function is f, \n",
    "    f: x -> f(x)\n",
    "    this preoduces a copy whose dynamic function is \n",
    "    F: (x, C) -> (f(x), dC/dt)\n",
    "    \n",
    "    cost_function is a torch function of f(x), x\n",
    "    giving the gradient of the cost\n",
    "    \n",
    "    cost_function(f(x), x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ControlNN = copy.deepcopy(DynamicNN)\n",
    "    \n",
    "    def modified_forward(z):\n",
    "        \"\"\"\n",
    "        Here we define a function that will replace \n",
    "        the forward method\n",
    "        \"\"\"\n",
    "        x = z.narrow(-1, 0, x_size)\n",
    "        \n",
    "        dyn_f = ControlNN.f(x)\n",
    "        \n",
    "        #below is the cost dynamical function\n",
    "        cost_f = cost_function(ControlNN.f(x), x).unsqueeze(0)\n",
    "        #match the 0th dimension of dyn_f\n",
    "        \n",
    "        if len(dyn_f.shape)==1:\n",
    "            #then we have a simple vector\n",
    "            shaped_cost_f = cost_f\n",
    "        else:\n",
    "            #then we have a tensor of shape [num_vecs, length_of_vec]\n",
    "            shaped_cost_f = cost_f.repeat(dyn_f.shape[0], 1)\n",
    "        \n",
    "        return torch.cat([dyn_f, shaped_cost_f], dim=-1)\n",
    "    \n",
    "    \n",
    "    ControlNN.forward = modified_forward\n",
    "    \n",
    "    return ControlNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing this is our dynamical function, we then have\n",
    "\n",
    "$$\n",
    "\\frac{d x}{d t} = M.x\n",
    "$$\n",
    "\n",
    "We'll have our goal to be to get $x$ to $x'$ after time T\n",
    "\n",
    "So we have a single data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now well build a class that implements the adjoint method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjointMethod():\n",
    "    def __init__(self, DynamicNN, ODESolver):\n",
    "        \"\"\"\n",
    "        this will implement the augmented adjoint \n",
    "        method\n",
    "        \n",
    "        DynamicNN is of the DynamicNN class \n",
    "        \n",
    "        loss_func: x-> Reals\n",
    "        \n",
    "        ODESolver is a function of the form\n",
    "        ODESolver(t0, t1, x0, f, reverse=False)\n",
    "        \"\"\"\n",
    "        self.nn = DynamicNN\n",
    "        self.ODESolver = ODESolver\n",
    "        #next we define the function which returns the dynamics of the position\n",
    "        self.func = lambda x, t: self.nn.forward(torch.from_numpy(x).float()).detach().numpy()\n",
    "        self.target_set=False\n",
    "        self.param_num=None\n",
    "        \n",
    "    def forward(self, t0, t1, x0):\n",
    "        \"\"\"\n",
    "        calculate the forward pass, obtain x1\n",
    "        \"\"\"\n",
    "        self.t0=t0\n",
    "        self.t1=t1\n",
    "        self.x0= x0.astype(float)\n",
    "        \n",
    "        \"\"\"\n",
    "        #next we define the function which returns the dynamics of the position\n",
    "        self.func = lambda x, t: self.nn.forward(torch.from_numpy(x).float()).detach().numpy()\n",
    "        \"\"\"\n",
    "        \n",
    "        x1=self.ODESolver(t0, t1, x0.astype(float), self.func, reverse=False)\n",
    "        \n",
    "        #print(EulerSolve(t0, t1, x0, self.func, dt = 0.01, reverse=False))\n",
    "        \n",
    "        self.x1 = x1\n",
    "        \n",
    "        return x1\n",
    "    \n",
    "    def dynamical_function(self, x,t):\n",
    "        \"\"\"\n",
    "        runs dynamical function on x, t\n",
    "        without using gradients\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            vec = self.func(x, t)\n",
    "        return vec\n",
    "    \n",
    "    def set_target(self, target, loss_function):\n",
    "        \"\"\"\n",
    "        target is a numpy array, or pytorch tensor\n",
    "        loss function is a pytorch function\n",
    "        \n",
    "        set the target, and the loss function\n",
    "        the loss function is a function of two points (x1, x2)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #make sure type of x0 is a tensor\n",
    "        self.target = torch.tensor(target).float()\n",
    "        \n",
    "        #set loss function\n",
    "        self.loss_func = lambda x: loss_function(x, self.target)\n",
    "        \n",
    "        self.target_set=True\n",
    "        \n",
    "    \n",
    "    def GiveDynamical(self):\n",
    "        \"\"\"\n",
    "        d[z, a, a_p]/dt = [f, -a f_z, -a f_p]\n",
    "        \n",
    "        here we return a function which gives the rhs\n",
    "        \"\"\"\n",
    "        \n",
    "        def AdjointDynamical(a_aug, t):\n",
    "            \"\"\"\n",
    "            here a_aug is given by \n",
    "            [z, a, a_p]\n",
    "            and evolves according to \n",
    "            d[z, a, a_p]/dt = [f, -a f_z, -a f_p]\n",
    "            \n",
    "            When this is implemented, we run it backwards\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            x = a_aug[:self.x0.shape[0]]\n",
    "            tensor_x = torch.tensor(x).float()\n",
    "            a = a_aug[self.x0.shape[0]:2*self.x0.shape[0]]\n",
    "            ap = a_aug[2*self.x0.shape[0]]\n",
    "            \n",
    "            assert(a.shape[-1]==x.shape[-1])\n",
    "            \n",
    "            f_p = self.nn.get_parameter_jacobian(tensor_x, out_type='numpy')\n",
    "            af_p = a @ f_p\n",
    "            \n",
    "            f_z = self.nn.get_jacobian(tensor_x, out_type='numpy')\n",
    "            af_z = a @ f_z\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                z_t = self.func(x, t)\n",
    "            \n",
    "            return np.concatenate([z_t, -af_z, -af_p])\n",
    "        \n",
    "        return AdjointDynamical\n",
    "    \n",
    "    def SetInitialConditions(self):\n",
    "        \"\"\"\n",
    "        assuming forward has been run, this loads the initial conditions\n",
    "        for d[a, a_p]/dt = [-a f_z, -a f_p]\n",
    "        \n",
    "        note this is run backwards, from t1 to t0\n",
    "        \n",
    "        the initial conditions are [dL(z1)/d z1, 0]\n",
    "        since the loss is defined at L(z1)\n",
    "        \"\"\"\n",
    "        \n",
    "        #first obtain a0 = dl/dz\n",
    "        x1_tensor = torch.tensor(self.x1).float().requires_grad_()\n",
    "        \n",
    "        loss = self.loss_func(x1_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        dldz = (x1_tensor.grad.data).detach().numpy()\n",
    "        \n",
    "        #print('dldz',dldz)\n",
    "        \n",
    "        assert(dldz.shape == self.x1.shape)\n",
    "        \n",
    "        #now a_p0 = 0, so we need a zero array with length equal to \n",
    "        #the number of parameters\n",
    "        if not self.param_num:\n",
    "            self.param_num=0\n",
    "            for param in self.nn.parameters():\n",
    "                self.param_num+=reduce(operator.mul, param.shape, 1)\n",
    "        \n",
    "        a_p0 = np.zeros(self.param_num)\n",
    "        \n",
    "        #put it all together\n",
    "        self.InitialConditions = np.concatenate([self.x1, dldz, a_p0])\n",
    "        \n",
    "        return self.InitialConditions\n",
    "    \n",
    "    def AjointSensitivity(self, t0, t1, x0, lr):\n",
    "        \"\"\"\n",
    "        here we'll enact the adjoint method to determine the gradient \n",
    "        we'll do this by solving the adjoint differential equation\n",
    "        \n",
    "        we also use initial conditions\n",
    "        this requires an initial pass of forward to save x1 in memory\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #assert that the target and the loss function\n",
    "        #have been set\n",
    "        assert(self.target_set)\n",
    "        \n",
    "        #first we do a forward pass\n",
    "        #this determines the x1 point\n",
    "        with torch.no_grad():\n",
    "            self.forward(t0, t1, x0)\n",
    "        \n",
    "        #now obtain the dynamical function telling us how\n",
    "        #to solve the adjoint ode\n",
    "        dynamic_f = self.GiveDynamical()\n",
    "        \n",
    "        #set the initial conditions for the adjoint\n",
    "        #dynamics\n",
    "        self.SetInitialConditions()\n",
    "        \n",
    "        #Next solve the adjoint ode\n",
    "        a0 = self.ODESolver(\n",
    "            t0, \n",
    "            t1, \n",
    "            self.InitialConditions, \n",
    "            dynamic_f,  \n",
    "            reverse=True)\n",
    "        \n",
    "        #a0 is of the form [z(t0), dL/dz(t0), dL/dp]\n",
    "        #now all but the first 2*z.shape[0] are dL/dp\n",
    "        flat_dldp = a0[2*self.x0.shape[0]:]\n",
    "        \n",
    "        #now we set the parameter grads to reflect the above gradient\n",
    "        self.nn.unflatten_param_grads(flat_dldp)\n",
    "        \n",
    "        #now we descend\n",
    "        self.nn.descend(lr)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        shortcut to clear grads from neural network\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.zero_()\n",
    "    \n",
    "    def get_loss(self):\n",
    "        \"\"\"\n",
    "        returns scalar of loss\n",
    "        assumes forward has been run, and target has been set\n",
    "        \"\"\"\n",
    "        assert(self.target_set)\n",
    "        \n",
    "        return self.loss_func(torch.tensor(self.x1).float()).detach().numpy()\n",
    "    \n",
    "    def copy_model(self):\n",
    "        \"\"\"\n",
    "        return a copy of the model\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(xmin, xmax, ymin, ymax, dynamic_f):\n",
    "    \"\"\"\n",
    "    dynamic_f is a function of x,y\n",
    "    returns a 2d vector\n",
    "    \"\"\"\n",
    "    \n",
    "    Y, X = np.mgrid[ymin:ymax:100j, xmin:xmax:100j]\n",
    "    \n",
    "    U, V = np.zeros((100, 100)), np.zeros((100, 100))\n",
    "    \n",
    "    for i, x in enumerate(X[0]):\n",
    "        for j, y in enumerate(Y[0]):\n",
    "            U[i, j], V[i, j] = dynamic_f(x, y)\n",
    "\n",
    "    plt.streamplot(X, Y, U, V, color=U, linewidth=2, cmap=plt.cm.cool)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22716\\303902208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDynamicNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mctnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mControlDynamicNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mctnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cost_function' is not defined"
     ]
    }
   ],
   "source": [
    "tnn = DynamicNN()\n",
    "\n",
    "ctnn = ControlDynamicNN(tnn, cost_function, 2)\n",
    "\n",
    "ctnn.forward(torch.randn(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnn = DynamicNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = lambda f, x: torch.norm(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctnn = ControlDynamicNN(tnn, cost_function, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctnn.forward(torch.tensor([1., 2., 33333.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define adjoint method object\n",
    "adjm = AdjointMethod(ctnn, EulerSolve)\n",
    "#set target and loss\n",
    "target = np.array([0.,1., np.pi])\n",
    "loss_function = nn.functional.mse_loss\n",
    "adjm.set_target(target, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream(-2, 2, -2, 2, lambda x, y: adjm.dynamical_function(np.array([x,y])[:2], 1))\n",
    "stream(-2, 2, -2, 2, lambda x, y: adjm.dynamical_function(np.array([x,y]), 1)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjm.dynamical_function(np.array([1, 2, 2]) ,1)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nn.functional.mse_loss(input=torch.Tensor([1, 2,3]), target=torch.Tensor([1, 2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set learning parameters\n",
    "t0=0\n",
    "t1=1\n",
    "#initial point\n",
    "x0=np.array([1., 0., 0])\n",
    "lr = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#procedure\n",
    "first_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for rep in range(500):\n",
    "    adjm.AjointSensitivity(t0, t1, x0, lr)\n",
    "    adjm.zero_grad() #this may not be necessary, not sure\n",
    "    #but it doesnt hurt\n",
    "    losses.append(adjm.get_loss())\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    xs, ys=EulerODEPath(\n",
    "        t0, \n",
    "        10, \n",
    "        np.array([1., 0.]), \n",
    "        lambda x, t: adjm.dynamical_function(x, t)[:2], \n",
    "        dt = 0.01, \n",
    "        reverse=False)\n",
    "    \n",
    "    ax.plot(xs, ys)\n",
    "    ax.scatter([1, 0],[0, 1], c='red')\n",
    "    ax.set_xlim(-1, 1.5)\n",
    "    ax.set_ylim(-1, 1.5)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    fig.savefig('paths/'+str(rep)+'.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "    \n",
    "final_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time per rep:',(final_time-first_time)/250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjm.x0)\n",
    "print(adjm.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream(0, 1.5, 0, 1.5, lambda x, y: adjm.dynamical_function(np.array([x,y]), 1)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in adjm.nn.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjm.x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = lambda x, t: adjm.dynamical_function(x, t)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys=EulerODEPath(t0, 10, np.array([1., 0.]), ff, dt = 0.01, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(xs, ys)\n",
    "ax.scatter([1, 0],[0, 1], c='red')\n",
    "ax.set_xlim(-1, 1.5)\n",
    "ax.set_ylim(-1, 1.5)\n",
    "\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
